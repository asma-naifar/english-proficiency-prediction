{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the NICT JLE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: explanation : regular expressions - used pattern for tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os import listdir\n",
    "\n",
    "# This function allows you to retrieve all the lines that are between the \"B\" tag and remove the new line\n",
    "def generateCleanFile(filename):\n",
    "    input = open(\"NICT_JLE_4.1/LearnerOriginal/\" + filename, \"r\", encoding=\"latin1\") \n",
    "    output = open(\"preProcessedData/\" + filename, \"w\", encoding=\"latin1\")\n",
    "    lines = input.readlines()\n",
    "    for line in lines:\n",
    "        if '<B>' in line :\n",
    "            output.write(line[3:len(line)-5])\n",
    "            #output.write(line[3:len(line)-5])\n",
    "    input.close()\n",
    "    output.close() \n",
    "\n",
    "# This function allows each line of each file to delete all tags and their data\n",
    "def generateCleanline(filename):\n",
    "    input = open(\"preProcessedData/\" + filename, \"r\", encoding=\"latin1\")\n",
    "    output = open(\"preProcessedDataLine/\" + filename, \"w\", encoding=\"latin1\")\n",
    "    lines = input.readlines()\n",
    "    for line in lines:\n",
    "            content = removeAllInternalTags(line)\n",
    "            output.write(content.lower())\n",
    "    input.close()\n",
    "    output.close() \n",
    "\n",
    "\n",
    "def removeAllInternalTags(line):\n",
    "    pile = list()\n",
    "    pile = [(m.start(0), m.end(0)) for m in re.finditer(r'<(.*?)>', line)]\n",
    "\n",
    "    buffer = []\n",
    "    counter = 0\n",
    "    while len(pile) != 0:\n",
    "        if line[pile[counter][0]+1] == '/':\n",
    "            line = line[0:buffer[-1][0]] + line[pile[counter][1]:]\n",
    "            pile = [(m.start(0), m.end(0)) for m in re.finditer(r'<(.*?)>', line)]\n",
    "            counter = 0\n",
    "            buffer.pop()\n",
    "        else :\n",
    "            buffer.append(pile[counter])\n",
    "            counter+=1\n",
    "    return line\n",
    "\n",
    "# Keep only signaficant tags that will help with the prediction of the SST level of each participant \n",
    "# tags is the list of tags that the user wants to remove from the transcript \n",
    "def removeInternalTags(line, tags=[]):\n",
    "    for tag in tags:\n",
    "        doubleTagPattern = \"<\",tag,\">(.+?)</\",tag,\">\"\n",
    "        singleTagPattern = \"</?\",tag,\"*?>\"\n",
    "        line = re.sub(doubleTagPattern, \"\", line)\n",
    "        line = re.sub(singleTagPattern, \"\", line)\n",
    "    return re.sub(r\"\\s+\", \" \", line)\n",
    "\n",
    "\n",
    "files = [f for f in listdir(\"NICT_JLE_4.1/LearnerOriginal\")]\n",
    "\n",
    "for f in files:\n",
    "    generateCleanFile(f)\n",
    "    \n",
    "for f in files:\n",
    "    generateCleanline(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the dataset: Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explanation : BoW algo - stopwords\n",
    "\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "    A vocabulary of known words.\n",
    "    A measure of the presence of known words.\n",
    "\n",
    "It is called a bag-of-words , because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document. The complexity comes both in deciding how to design the vocabulary of known words (or tokens) and how to score the presence of known words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Associate text file and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "def extractScore(filename):\n",
    "    score = {}\n",
    "    input = open(\"NICT_JLE_4.1/LearnerOriginal/\" + filename, \"r\", encoding=\"latin1\") \n",
    "    lines = input.readlines()\n",
    "    for line in lines:\n",
    "        if '<SST_level>' in line :\n",
    "            score = line[11]\n",
    "            break\n",
    "    input.close()\n",
    "    return score\n",
    "\n",
    "files = [f for f in listdir(\"NICT_JLE_4.1/LearnerOriginal\")]\n",
    "\n",
    "textScoreList = dict()\n",
    "\n",
    "for f in files:\n",
    "    textScoreList[f] = extractScore(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocabulary \n",
    "\n",
    "As a first step, we create the function for generating the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "import numpy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def createVectorizer(text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer\n",
    "\n",
    "def word_extraction(sentence):    \n",
    "    ignore = set(stopwords.words('english')) \n",
    "    words = sentence.split()\n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore] \n",
    "    return ' '.join(cleaned_text)\n",
    "    \n",
    "# def tokenize(sentences):    \n",
    "#     words = []    \n",
    "#     for sentence in sentences:        \n",
    "#         w = word_extraction(sentence)        \n",
    "#         words.extend(w)            \n",
    "#         words = sorted(list(set(words)))  \n",
    "#     return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing database to training and testing sets randomly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from math import floor\n",
    "import os\n",
    "\n",
    "allFiles = os.listdir(\"preProcessedDataLine/\")\n",
    "\n",
    "# Randomize training data\n",
    "shuffle(allFiles)\n",
    "\n",
    "split = 0.6\n",
    "\n",
    "split_index = floor(len(allFiles) * split)\n",
    "trainingFiles = allFiles[:split_index]\n",
    "testingFiles = allFiles[split_index:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating vocab from training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingText = list()\n",
    "testingText = list()\n",
    "\n",
    "for trainingFile in trainingFiles:\n",
    "    trainingFileText = open(\"preProcessedDataLine/\" + trainingFile, \"r\", encoding=\"latin1\")\n",
    "    l = trainingFileText.readline()\n",
    "    trainingText.append(word_extraction(l))\n",
    "    trainingFileText.close()\n",
    "\n",
    "for testingFile in testingFiles:\n",
    "    testingFileText = open(\"preProcessedDataLine/\" + testingFile, \"r\", encoding=\"latin1\")\n",
    "    l = testingFileText.readline()\n",
    "    testingText.append(word_extraction(l))\n",
    "    testingFileText.close()\n",
    "\n",
    "vectorizer = createVectorizer(trainingText)\n",
    "\n",
    "trainingVector = vectorizer.transform(trainingText) \n",
    "testingVector = vectorizer.transform(testingText) \n",
    "\n",
    "trainingData = trainingVector.toarray() \n",
    "testingData = testingVector.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate BoW vectors for training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# dataframe for training data : inputs\n",
    "x_train = pd.DataFrame(trainingData, columns = sorted(vectorizer.vocabulary_))\n",
    "x_train.index = trainingFiles\n",
    "\n",
    "# dataframe for training data : scores\n",
    "y_train = pd.DataFrame([textScoreList[trainingFile] for trainingFile in trainingFiles], columns = [\"score\"])\n",
    "y_train.index = trainingFiles\n",
    "\n",
    "\n",
    "# dataframe for testing data : inputs\n",
    "x_test = pd.DataFrame(testingData, columns = sorted(vectorizer.vocabulary_))\n",
    "x_test.index = testingFiles\n",
    "\n",
    "# dataframe for testing data : scores\n",
    "y_test = pd.DataFrame([textScoreList[testingFile] for testingFile in testingFiles], columns = [\"score\"])\n",
    "y_test.index = testingFiles\n",
    "\n",
    "print(x_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifier to Predict SST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the \"process\" of choosing a classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9851\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [<class 'keras.engine.data_adapter.TensorLikeDataAdapter'>, <class 'keras.engine.data_adapter.GeneratorDataAdapter'>] to handle input: <class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14712/2370162264.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m               metrics=['score'])\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    991\u001b[0m             _type_name(x), _type_name(y)))\n\u001b[0;32m    992\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m     raise RuntimeError(\n\u001b[0m\u001b[0;32m    994\u001b[0m         \u001b[1;34m\"Data adapters should be mutually exclusive for \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;34m\"handling inputs. Found multiple adapters {} to handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Data adapters should be mutually exclusive for handling inputs. Found multiple adapters [<class 'keras.engine.data_adapter.TensorLikeDataAdapter'>, <class 'keras.engine.data_adapter.GeneratorDataAdapter'>] to handle input: <class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(len(sorted(vectorizer.vocabulary_)),)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(9, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,epochs=400, batch_size=128, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4ee4038f3ce1aaa32f3129e506029845ba1ff8163a79a51eba3ccd17ed5d0cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
