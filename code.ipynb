{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the NICT JLE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: explanation : regular expressions - used pattern for tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os import listdir\n",
    "\n",
    "# This function allows you to retrieve all the lines that are between the \"B\" tag and remove the new line\n",
    "def generateCleanFile(filename):\n",
    "    input = open(\"NICT_JLE_4.1/LearnerOriginal/\" + filename, \"r\", encoding=\"latin1\") \n",
    "    output = open(\"preProcessedData/\" + filename, \"w\", encoding=\"latin1\")\n",
    "    lines = input.readlines()\n",
    "    for line in lines:\n",
    "        if '<B>' in line :\n",
    "            output.write(line[3:len(line)-5])\n",
    "            #output.write(line[3:len(line)-5])\n",
    "    input.close()\n",
    "    output.close() \n",
    "\n",
    "# This function allows each line of each file to delete all tags and their data\n",
    "def generateCleanline(filename):\n",
    "    input = open(\"preProcessedData/\" + filename, \"r\", encoding=\"latin1\")\n",
    "    output = open(\"preProcessedDataLine/\" + filename, \"w\", encoding=\"latin1\")\n",
    "    lines = input.readlines()\n",
    "    for line in lines:\n",
    "            content = removeAllInternalTags(line)\n",
    "            output.write(content.lower())\n",
    "    input.close()\n",
    "    output.close() \n",
    "\n",
    "\n",
    "def removeAllInternalTags(line):\n",
    "    pile = list()\n",
    "    pile = [(m.start(0), m.end(0)) for m in re.finditer(r'<(.*?)>', line)]\n",
    "\n",
    "    buffer = []\n",
    "    counter = 0\n",
    "    while len(pile) != 0:\n",
    "        if line[pile[counter][0]+1] == '/':\n",
    "            line = line[0:buffer[-1][0]] + line[pile[counter][1]:]\n",
    "            pile = [(m.start(0), m.end(0)) for m in re.finditer(r'<(.*?)>', line)]\n",
    "            counter = 0\n",
    "            buffer.pop()\n",
    "        else :\n",
    "            buffer.append(pile[counter])\n",
    "            counter+=1\n",
    "    return line\n",
    "\n",
    "# Keep only signaficant tags that will help with the prediction of the SST level of each participant \n",
    "# tags is the list of tags that the user wants to remove from the transcript \n",
    "def removeInternalTags(line, tags=[]):\n",
    "    for tag in tags:\n",
    "        doubleTagPattern = \"<\",tag,\">(.+?)</\",tag,\">\"\n",
    "        singleTagPattern = \"</?\",tag,\"*?>\"\n",
    "        line = re.sub(doubleTagPattern, \"\", line)\n",
    "        line = re.sub(singleTagPattern, \"\", line)\n",
    "    return re.sub(r\"\\s+\", \" \", line)\n",
    "\n",
    "\n",
    "files = [f for f in listdir(\"NICT_JLE_4.1/LearnerOriginal\")]\n",
    "\n",
    "for f in files:\n",
    "    generateCleanFile(f)\n",
    "    \n",
    "for f in files:\n",
    "    generateCleanline(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the dataset: extract features with the Bag of Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explanation : BoW algo - stopwords\n",
    "\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "    A vocabulary of known words.\n",
    "    A measure of the presence of known words.\n",
    "\n",
    "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "import numpy\n",
    "import re\n",
    "\n",
    "def createVocab():\n",
    "    files = [f for f in listdir(\"preProcessedDataLine/\")]\n",
    "    vocab = set()\n",
    "    for filename in files:\n",
    "        f = open(\"preProcessedDataLine/\" + filename, \"r\", encoding=\"latin1\")\n",
    "        vocab.update(tokenize(f))\n",
    "        f.close()\n",
    "    return vocab\n",
    "\n",
    "def word_extraction(sentence):    \n",
    "    ignore = set(stopwords.words('english'))   \n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]    \n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize(sentences):    \n",
    "    words = []    \n",
    "    for sentence in sentences:        \n",
    "        w = word_extraction(sentence)        \n",
    "        words.extend(w)            \n",
    "        words = sorted(list(set(words)))  \n",
    "    return words\n",
    "\n",
    "def generate_bow(filename, vocab):\n",
    "    f = open(\"preProcessedDataLine/\" + filename, \"r\", encoding=\"latin1\") \n",
    "    lines = f.readlines()\n",
    "    vector = list()        \n",
    "    for line in lines:        \n",
    "        words = word_extraction(line)        \n",
    "        vector = numpy.zeros(len(vocab))        \n",
    "        for w in words:            \n",
    "            for i,word in enumerate(vocab):                \n",
    "                if word == w:                     \n",
    "                    vector[i] += 1 \n",
    "    f.close()                 \n",
    "    return vector\n",
    "\n",
    "    \n",
    "\n",
    "#files = [f for f in listdir(\"preProcessedDataLine\")]\n",
    "vocab = createVocab()\n",
    "print(generate_bow(\"file01281.txt\", vocab))\n",
    "\n",
    "\n",
    "# TODO : save the BoW output in a variable as our input for the classifier \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifier to Predict SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4ee4038f3ce1aaa32f3129e506029845ba1ff8163a79a51eba3ccd17ed5d0cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
