{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the NICT JLE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: explanation : regular expressions - used pattern for tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "def generateCleanFile(filename):\n",
    "    input = open(\"NICT_JLE_4.1/LearnerOriginal/\" + filename, \"r\", encoding=\"latin1\") \n",
    "    output = open(\"preProcessedData/\" + filename, \"w\", encoding=\"latin1\")\n",
    "    lines = input.readlines()\n",
    "    for line in lines:\n",
    "        if '<B>' in line :\n",
    "            output.write(removeAllInternalTags(line[3:len(line)-5])+\"\\n\")\n",
    "    input.close()\n",
    "    output.close() \n",
    "\n",
    "def removeAllInternalTags(line):\n",
    "    pile = list()\n",
    "    pile = [(m.start(0), m.end(0)) for m in re.finditer(r'<(.*?)>', line)]\n",
    "\n",
    "    buffer = []\n",
    "    counter = 0\n",
    "    while len(pile) != 0:\n",
    "        if line[pile[counter][0]+1] == '/':\n",
    "            line = line[0:buffer[-1][0]] + line[pile[counter][1]:]\n",
    "            pile = [(m.start(0), m.end(0)) for m in re.finditer(r'<(.*?)>', line)]\n",
    "            counter = 0\n",
    "            buffer.pop()\n",
    "        else :\n",
    "            buffer.append(pile[counter])\n",
    "            counter+=1\n",
    "    return line\n",
    "\n",
    "# Keep only signaficant tags that will help with the prediction of the SST level of each participant \n",
    "# tags is the list of tags that the user wants to remove from the transcript \n",
    "def removeInternalTags(line, tags=[]):\n",
    "    for tag in tags:\n",
    "        doubleTagPattern = \"<\",tag,\">(.+?)</\",tag,\">\"\n",
    "        singleTagPattern = \"</?\",tag,\"*?>\"\n",
    "        line = re.sub(doubleTagPattern, \"\", line)\n",
    "        line = re.sub(singleTagPattern, \"\", line)\n",
    "    return re.sub(r\"\\s+\", \" \", line)\n",
    "\n",
    "\n",
    "files = [f for f in listdir(\"NICT_JLE_4.1/LearnerOriginal\")]\n",
    "\n",
    "#generateCleanFile(\"file00046.txt\")\n",
    "\n",
    "for f in files:\n",
    "    generateCleanFile(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the dataset: extract features with the Bag of Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: explanation : BoW algo - stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "import numpy\n",
    "import re\n",
    "\n",
    "\n",
    "def word_extraction(sentence):    \n",
    "    ignore = set(stopwords.words('english'))   \n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]    \n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize(sentences):    \n",
    "    words = []    \n",
    "    for sentence in sentences:        \n",
    "        w = word_extraction(sentence)        \n",
    "        words.extend(w)            \n",
    "        words = sorted(list(set(words)))    \n",
    "    return words\n",
    "\n",
    "def generate_bow(filename):\n",
    "    f = open(\"preProcessedData/\" + filename, \"r\", encoding=\"latin1\") \n",
    "    lines = f.readlines()\n",
    "    vocab = tokenize(lines)\n",
    "    vector = list()        \n",
    "    for line in lines:        \n",
    "        words = word_extraction(line)        \n",
    "        bag_vector = numpy.zeros(len(vocab))        \n",
    "        for w in words:            \n",
    "            for i,word in enumerate(vocab):                \n",
    "                if word == w:                     \n",
    "                    bag_vector[i] += 1 \n",
    "    f.close()                 \n",
    "    return vector\n",
    "\n",
    "    \n",
    "\n",
    "files = [f for f in listdir(\"preProcessedData\")]\n",
    "\n",
    "# TODO : save the BoW output in a variable as our input for the classifier \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifier to Predict SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4ee4038f3ce1aaa32f3129e506029845ba1ff8163a79a51eba3ccd17ed5d0cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
