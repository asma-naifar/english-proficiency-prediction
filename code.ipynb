{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing the NICT JLE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "def generateCleanFile(filename):\n",
    "    input = open(\"NICT_JLE_4.1/LearnerOriginal/\" + filename, \"r\", encoding=\"latin1\") \n",
    "    output = open(\"preProcessedData/\" + filename, \"w\", encoding=\"latin1\")\n",
    "    lines = input.readlines()\n",
    "    for line in lines:\n",
    "        if '<B>' in line :\n",
    "            output.write(removeInternalTags(line[3:len(line)-5])+\"\\n\")\n",
    "    input.close()\n",
    "    output.close() \n",
    "\n",
    "# TODO: Keep signaficant tags that will help with the prediction of the SST level of each participant \n",
    "def removeInternalTags(line):\n",
    "    doubleTagPattern = \"<.*?>(.+?)</.*?>\"\n",
    "    singleTagPattern = \"</?.*?>\"\n",
    "    line = re.sub(doubleTagPattern, \"\", line)\n",
    "    line = re.sub(singleTagPattern, \"\", line)\n",
    "    return re.sub(r\"\\s+\", \" \", line)\n",
    "\n",
    "files = [f for f in listdir(\"NICT_JLE_4.1/LearnerOriginal\")]\n",
    "\n",
    "for f in files:\n",
    "    generateCleanFile(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the dataset: extract features with the Bag of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab 168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "168\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "import numpy\n",
    "import re\n",
    "\n",
    "\n",
    "def word_extraction(sentence):    \n",
    "    ignore = set(stopwords.words('english'))   \n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]    \n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize(sentences):    \n",
    "    words = []    \n",
    "    for sentence in sentences:        \n",
    "        w = word_extraction(sentence)        \n",
    "        words.extend(w)            \n",
    "        words = sorted(list(set(words)))    \n",
    "    return words\n",
    "\n",
    "def generate_bow(filename):\n",
    "    f = open(\"preProcessedData/\" + filename, \"r\", encoding=\"latin1\") \n",
    "    lines = f.readlines()\n",
    "    vocab = tokenize(lines)\n",
    "    print(\"vocab\",len(vocab))\n",
    "    vector = list()        \n",
    "    for line in lines:        \n",
    "        words = word_extraction(line)        \n",
    "        bag_vector = numpy.zeros(len(vocab))        \n",
    "        for w in words:            \n",
    "            for i,word in enumerate(vocab):                \n",
    "                if word == w:                     \n",
    "                    bag_vector[i] += 1 \n",
    "        print(len(bag_vector))         \n",
    "    f.close()                 \n",
    "    return vector\n",
    "\n",
    "    \n",
    "\n",
    "files = [f for f in listdir(\"preProcessedData\")]\n",
    "\n",
    "# TODO : save the BoW output in a variable as our input for the classifier \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the classifier to Predict SST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and Confusion matrix"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4ee4038f3ce1aaa32f3129e506029845ba1ff8163a79a51eba3ccd17ed5d0cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
